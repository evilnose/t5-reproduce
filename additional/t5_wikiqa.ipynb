{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "colab": {
      "name": "t5-wikiqa.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1615688287613
        },
        "id": "ki7-dhMhenXh"
      },
      "source": [
        "import os\r\n",
        "from collections import Counter\r\n",
        "import string\r\n",
        "import re\r\n",
        "import json\r\n",
        "import sys\r\n",
        "\r\n",
        "import transformers\r\n",
        "from transformers import AutoTokenizer\r\n",
        "from transformers import T5Config, T5ForConditionalGeneration \r\n",
        "from datasets import load_dataset, load_metric\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from transformers import Adafactor\r\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl8DeqtRBw8N"
      },
      "source": [
        "## Load WebQA Dataset\r\n",
        "Load and split the dataset. 10% of the training set is allocated as validation.\r\n",
        "\r\n",
        "*Note*: To test for ablation, simply change `'train[:90%]'` to a lower value,\r\n",
        "e.g. `'train[:45%]'` for half the original size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688287872
        },
        "id": "eXgOq4QqenXh"
      },
      "source": [
        "# Download and split the dataset\r\n",
        "TRAIN_IDX = 0\r\n",
        "VAL_IDX = 1\r\n",
        "TEST_IDX = 2\r\n",
        "dataset = load_dataset('wiki_qa', split=['train[:90%]', 'train[-10%:]', 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5z3myRBCbOu"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688288709
        },
        "id": "fYKVV4gmenXi"
      },
      "source": [
        "DROPOUT = 0.05  # Dropout; 0.05 in original paper\r\n",
        "LR = 1e-3  # Learning rate; 1e-3 in original paper\r\n",
        "BATCH_SIZE = 8  # Training batch size\r\n",
        "VAL_BATCH_SIZE = 8  # Validation batch size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4SIQQqpK-ep"
      },
      "source": [
        "MODEL_NAME = 't5-small'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8Jywz0CDAo"
      },
      "source": [
        "## Preprocessing\r\n",
        "* Load tokenizer\r\n",
        "* Tokenize and pad in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688288534
        },
        "id": "-ZaJmvK-enXi"
      },
      "source": [
        "# Load the tokenizer\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688314283
        },
        "id": "19bnWBl8enXj"
      },
      "source": [
        "def prepare_examples(examples):\r\n",
        "    '''Passed to Dataset.map. Tokenize and pad in batches; pick first answer'''\r\n",
        "\r\n",
        "    # Tokenize questions\r\n",
        "    tokenized_examples = tokenizer(\r\n",
        "        examples[\"question\"],\r\n",
        "        padding=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Tokenize target as 'label_ids'\r\n",
        "    tokenized_examples['label_ids'] = tokenizer(examples['answer'],\r\n",
        "                                                padding=True).input_ids\r\n",
        "    return tokenized_examples\r\n",
        "\r\n",
        "def get_tokenized(dataset, idx, batch_size):\r\n",
        "    '''Get the preprocessed (tokenized) dataset.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        dataset:    The dataset to preprocess.\r\n",
        "        idx:        The index of the dataset (one of TRAIN_IDX, VAL_IDX, TEST_IDX)\r\n",
        "        batch_size: The batch size.\r\n",
        "    '''\r\n",
        "    return dataset[idx].map(prepare_examples,\r\n",
        "                            batched=True,\r\n",
        "                            batch_size=batch_size,\r\n",
        "                            remove_columns=dataset[idx].column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688317649
        },
        "id": "sn3nDPkBenXj"
      },
      "source": [
        "# Filter out answers with label 0\r\n",
        "dataset[TRAIN_IDX] = dataset[TRAIN_IDX].filter(lambda x: x['label'] == 1)\r\n",
        "dataset[VAL_IDX] = dataset[VAL_IDX].filter(lambda x: x['label'] == 1)\r\n",
        "\r\n",
        "# Tokenize datasets\r\n",
        "tokenized_train = get_tokenized(dataset, TRAIN_IDX, BATCH_SIZE)\r\n",
        "tokenized_val = get_tokenized(dataset, VAL_IDX, VAL_BATCH_SIZE)\r\n",
        "\r\n",
        "# Set the token ids to torch tensors\r\n",
        "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label_ids'])\r\n",
        "tokenized_val.set_format('torch', columns=['input_ids', 'attention_mask', 'label_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMFDPjtoDco8"
      },
      "source": [
        "## Load T5 Model Checkpoint\r\n",
        "* Load config separately to update dropout rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688314079
        },
        "id": "KK3tGMX9enXi"
      },
      "source": [
        "# Load config; override dropout\r\n",
        "# Overwrite task_specific_params since they are irrelevant to this task and\r\n",
        "# may cause some obscure error\r\n",
        "config = T5Config.from_pretrained(MODEL_NAME, dropout_rate=DROPOUT, task_specific_params=dict())\r\n",
        "\r\n",
        "# Load pretrained model with config\r\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, config=config)\r\n",
        "\r\n",
        "assert model.config.dropout_rate == DROPOUT\r\n",
        "\r\n",
        "# Change device of model\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qBcAMPeFAqX"
      },
      "source": [
        "## Utility Functions\r\n",
        "* Get remaining memory\r\n",
        "* Metrics\r\n",
        "* Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688318248
        },
        "id": "RloxNXIcenXj"
      },
      "source": [
        "def get_memory():\r\n",
        "    '''Get remaining GPU memory in bytes; used to test for memory leak.'''\r\n",
        "    t = torch.cuda.get_device_properties(0).total_memory\r\n",
        "    r = torch.cuda.memory_reserved(0) \r\n",
        "    a = torch.cuda.memory_allocated(0)\r\n",
        "    f = r - a  # free inside reserved\r\n",
        "    return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688318856
        },
        "id": "xHU2O-d9enXk"
      },
      "source": [
        "'''Adapted from the SQUAD evaluation script:\r\n",
        "https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py'''\r\n",
        "\r\n",
        "exclude = set(string.punctuation)\r\n",
        "\r\n",
        "def normalize_answer(s):\r\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\r\n",
        "    def remove_articles(text):\r\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\r\n",
        "\r\n",
        "    def white_space_fix(text):\r\n",
        "        return ' '.join(text.split())\r\n",
        "\r\n",
        "    def remove_punc(text):\r\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\r\n",
        "\r\n",
        "    def lower(text):\r\n",
        "        return text.lower()\r\n",
        "\r\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n",
        "\r\n",
        "\r\n",
        "def f1_score(prediction, ground_truth):\r\n",
        "    '''Return the F1 score given prediction and ground truth strings.\r\n",
        "    \r\n",
        "    The F1 score is token-based.\r\n",
        "    '''\r\n",
        "    prediction_tokens = normalize_answer(prediction).split()\r\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\r\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\r\n",
        "    num_same = sum(common.values())\r\n",
        "    if num_same == 0:\r\n",
        "        return 0\r\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\r\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\r\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\r\n",
        "    return f1\r\n",
        "\r\n",
        "def exact_match_score(prediction, ground_truth):\r\n",
        "    '''Return the exact match score given prediction and ground truth strings.\r\n",
        "    \r\n",
        "    1 is returned if two strings match exactly; 0 otherwise.\r\n",
        "    '''\r\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(gold_answers, predictions):\r\n",
        "    '''Return F1 and exact match score, given the gold answers and predictions.\r\n",
        "\r\n",
        "    The maximum is taken among all the possible answers.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        gold_answers:   A list of correct answer strings.\r\n",
        "        predictions:    A list of predicted answer strings.\r\n",
        "    '''\r\n",
        "    f1 = exact_match = total = 0\r\n",
        "\r\n",
        "    for gt, prediction in zip(gold_answers, predictions):\r\n",
        "      total += 1\r\n",
        "      exact_match += exact_match_score(prediction, gt)\r\n",
        "      f1 += f1_score(prediction, gt)\r\n",
        "    \r\n",
        "    exact_match = 100.0 * exact_match / total\r\n",
        "    f1 = 100.0 * f1 / total\r\n",
        "\r\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615688319124
        },
        "id": "beJ5vli-enXk"
      },
      "source": [
        "def evaluate_dataset(model, dataloader, dataset_idx):\r\n",
        "    '''Evaluate on an entire dataset.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        model:          The model to evaluate.\r\n",
        "        dataloader:     The DataLoader of the dataset.\r\n",
        "        dataset_idx:    One of {TRAIN/VAL/TEST}_IDX\r\n",
        "    '''\r\n",
        "    exact_match_sum = 0\r\n",
        "    f1_sum = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        index = 0\r\n",
        "        for batch in tqdm(dataloader):\r\n",
        "            batch_size = len(batch['input_ids'])\r\n",
        "\r\n",
        "            # Manually retrive list of answer strings for each question\r\n",
        "            answers = dataset[dataset_idx]['answer'][index : index + batch_size]\r\n",
        "            index += batch_size\r\n",
        "            \r\n",
        "            inputs = batch['input_ids'].to(device)\r\n",
        "            mask = batch['attention_mask'].to(device)\r\n",
        "            \r\n",
        "            # The padding does not really matter, since they will be skipped\r\n",
        "            # during decode anyway\r\n",
        "            outs = model.generate(input_ids=inputs, attention_mask=mask,\r\n",
        "                                  max_length=16, early_stopping=True)\r\n",
        "            \r\n",
        "            # Decode and skip special tokens such as padding.\r\n",
        "            outs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs] \r\n",
        "\r\n",
        "            result = evaluate(answers, outs)\r\n",
        "            exact_match_sum += result['exact_match']\r\n",
        "            f1_sum += result['f1']\r\n",
        "\r\n",
        "    return {\r\n",
        "        'exact_match': exact_match_sum / len(dataloader),\r\n",
        "        'f1': f1_sum / len(dataloader),\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ozmjbHH0gK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615530479228
        },
        "id": "cPr9XGMWenXl"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "model.train()\r\n",
        "\r\n",
        "train_loader = DataLoader(tokenized_train, batch_size=BATCH_SIZE)\r\n",
        "val_loader = DataLoader(tokenized_val, batch_size=VAL_BATCH_SIZE)\r\n",
        "\r\n",
        "optim = Adafactor(model.parameters(), lr=LR, relative_step=False)\r\n",
        "\r\n",
        "loss_list = list()\r\n",
        "f1_list = list()\r\n",
        "exact_match_list = list()\r\n",
        "\r\n",
        "for epoch in range(251):\r\n",
        "    cur_loss = 0.\r\n",
        "\r\n",
        "    # Train\r\n",
        "    for batch in tqdm(train_loader):\r\n",
        "        optim.zero_grad()\r\n",
        "        input_ids = batch['input_ids'].to(device)\r\n",
        "        label_ids = batch['label_ids'].to(device)\r\n",
        "        loss = model(input_ids, labels=label_ids).loss\r\n",
        "        cur_loss += loss.item()\r\n",
        "        loss.backward()\r\n",
        "        optim.step()\r\n",
        "\r\n",
        "    # Print and save every 10 epochs\r\n",
        "    if epoch % 10 == 0:\r\n",
        "        val_result = evaluate_dataset(model, val_loader, VAL_IDX)\r\n",
        "        # train_result = evaluate_dataset(model, train_loader, TRAIN_IDX)\r\n",
        "        elapsed = time.time() - t0\r\n",
        "        print('Epoch {3} - Loss: {0}; Val F1: {1}; Val exact match: {2}%;'\r\n",
        "        ' Elapsed {4:.2f} secs'.format(cur_loss,\r\n",
        "                                       val_result['f1'],\r\n",
        "                                       val_result['exact_match'],\r\n",
        "                                       epoch,\r\n",
        "                                       elapsed))\r\n",
        "        \r\n",
        "        # Record loss and metric for plotting later\r\n",
        "        loss_list.append(cur_loss)\r\n",
        "        f1_list.append(val_result['f1'])\r\n",
        "        exact_match_list.append(val_result['exact_match'])\r\n",
        "        torch.save({\r\n",
        "            'epoch': epoch,\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optim.state_dict(),\r\n",
        "            'loss_list': loss_list,\r\n",
        "            'f1_list': f1_list,\r\n",
        "            'exact_match_list': exact_match_list,\r\n",
        "            'elapsed': elapsed,\r\n",
        "            }, 'webqa-model-{}.pth'.format(epoch))\r\n",
        "    # print('Remaining memory: {:.2f} GB'.format(get_memory() / 1024 / 1024 / 1024))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyitpDwiH2oW"
      },
      "source": [
        "## Final Evaluation\r\n",
        "This is not really necessary except for the sake of clarity, since we already evaluate during training.\r\n",
        "\r\n",
        "The test set is not used, since the original paper uses validation only as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615572509912
        },
        "id": "dPJ5nYfQenXm"
      },
      "source": [
        "model.eval()\r\n",
        "val_result = evaluate_dataset(model, val_loader, VAL_IDX)\r\n",
        "print('Final validation results: {}'.format(val_result))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}